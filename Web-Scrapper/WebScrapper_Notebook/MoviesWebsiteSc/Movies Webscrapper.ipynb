{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a0c698",
   "metadata": {},
   "source": [
    "## Section 1: Scraping a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af533970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic (1997) - full transcript\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Collecting data from html is done by request\n",
    "#############################################\n",
    "\n",
    "\"\"\" \n",
    "Requesting the content from website using Requests. \n",
    "requests.get is for sending Response code\n",
    "We are not required the status code but we are requiring the content i.e. the text inside it so use .text\n",
    "\"\"\"\n",
    "\n",
    "website = 'https://subslikescript.com/movie/Titanic-120338'\n",
    "result = requests.get(website)\n",
    "content = result.text\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# BeautifulSoup is for to process pull data from html \n",
    "#####################################################\n",
    "\n",
    "\"\"\"\n",
    "Beautiful Soup is like a big bowl which organize content for fast extraction making scrapping task easy\n",
    "So our soup is main object where all operation has to perform\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(content,'lxml')\n",
    "# print(soup.prettify())\n",
    "\n",
    "\n",
    "title = soup.find('article',class_=\"main-article\")\n",
    "\n",
    "\n",
    "# print(title.h1.text.split()[:-3])\n",
    "name = title.h1.text.split()[:-3]\n",
    "\n",
    "\n",
    "movie_name = ' '.join(name)\n",
    "print(movie_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "#  Exporting data in a text file\n",
    "################################\n",
    "\n",
    "with open(f'{movie_name}.txt','w') as file:\n",
    "          file.write(movie_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b58f46",
   "metadata": {},
   "source": [
    "## Section 2: Scraping Multiple Transcripts and Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0447563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://subslikescript.com/movies?page=2\n",
      "https://subslikescript.com/movies?page=3\n",
      "https://subslikescript.com/movies?page=4\n",
      "https://subslikescript.com/movies?page=5\n",
      "https://subslikescript.com/movies?page=6\n",
      "https://subslikescript.com/movies?page=7\n",
      "https://subslikescript.com/movies?page=8\n",
      "https://subslikescript.com/movies?page=9\n",
      "https://subslikescript.com/movies?page=10\n",
      "https://subslikescript.com/movies?page=11\n",
      "https://subslikescript.com/movies?page=12\n",
      "https://subslikescript.com/movies?page=13\n",
      "https://subslikescript.com/movies?page=14\n"
     ]
    }
   ],
   "source": [
    "Movies_list = requests.get('https://subslikescript.com/movies').text\n",
    "soup1 = BeautifulSoup(Movies_list,'lxml')\n",
    "a = (soup1.find('a',class_='page-link').get('href'))\n",
    "print(a)\n",
    "\n",
    "\n",
    "def fun(link):\n",
    "    m = requests.get(link).text\n",
    "    soup = BeautifulSoup(m,'lxml')\n",
    "    next_pg_link = (soup.find('a',class_='page-link',rel= 'next').get('href'))\n",
    "    print(next_pg_link)\n",
    "    return (next_pg_link)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c = fun(a)\n",
    "for i in range(11):\n",
    "    c = fun(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afab918",
   "metadata": {},
   "source": [
    "### Now collecting movies name on single page \n",
    "Learn why we use find_all over find if we wanted to iterate over multiple similar type tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e125b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flaxy Martin (1949)\n",
      "Smart Girls Don't Talk (1948)\n",
      "Kieu (2021)\n",
      "Call Me Victor (1993)\n",
      "Eva & Adam (2021)\n",
      "The Loser Hero (2018)\n",
      "Andrew Santino: Cheeseburger (2023)\n",
      "My Sister's Serial Killer Boyfriend (2023)\n",
      "Yoba (1976)\n",
      "Trail to Terror (2016)\n",
      "Beautiful Minds (2021)\n",
      "Utama (2022)\n",
      "Savage Messiah (2002)\n",
      "Eddie Murphy: Delirious (1983)\n",
      "Gila! (2012)\n",
      "Bed Rest (2022)\n",
      "Ju-on: White Ghost (2009)\n",
      "D Block (2022)\n",
      "XxxHolic (2022)\n",
      "#DUPE# (2007)\n",
      "The Road to Denver (1955)\n",
      "Trail Street (1947)\n",
      "Bhaskar the Rascal (2015)\n",
      "The Hero (2017)\n",
      "The Meteor Man (1993)\n",
      "Minazuki (1999)\n",
      "Heartland of Darkness (1992)\n",
      "The Tin Soldier (1995)\n",
      "Joyland (2022)\n",
      "Reba McEntire's the Hammer (2023)\n"
     ]
    }
   ],
   "source": [
    "# Function to collect movies list of single page\n",
    "\n",
    "\n",
    "######################\n",
    "## Collecting movies name using find\n",
    "######################\n",
    "# u = soup1.find('ul',class_='scripts-list').a.li\n",
    "# print(u)\n",
    "\n",
    "\"\"\"We saw using find we are able to extract only single name of movie over multiple elements \n",
    "so we are using find_all bcoz it's not possible to use for loop for u here and extract movies name using a.li.text \n",
    "for i in u:\n",
    "    print(i.a.li) #this is not correct method bcoz i is iterating over each item in u\n",
    "\"\"\"   \n",
    "\n",
    "def finding_list():\n",
    "    for word in soup1.find_all('a'):\n",
    "        if word.li != None:\n",
    "            print(word.li.text)\n",
    "finding_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2d5f7",
   "metadata": {},
   "source": [
    "### Adding all above code in one block and creating a txt file with all movies name with link at top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda85f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://subslikescript.com/movies?page=2\n",
      "https://subslikescript.com/movies?page=3\n",
      "https://subslikescript.com/movies?page=4\n",
      "https://subslikescript.com/movies?page=5\n",
      "https://subslikescript.com/movies?page=6\n",
      "https://subslikescript.com/movies?page=7\n",
      "https://subslikescript.com/movies?page=8\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "\n",
    "Movies_list = requests.get('https://subslikescript.com/movies').text\n",
    "soup1 = BeautifulSoup(Movies_list,'lxml')\n",
    "a = (soup1.find('a',class_='page-link').get('href'))\n",
    "print(a)\n",
    "\n",
    "\n",
    "def fun(link):\n",
    "    m = requests.get(link).text\n",
    "    soup = BeautifulSoup(m,'lxml')\n",
    "    for word in soup.find_all('a'):\n",
    "        if word.li != None:\n",
    "            movieslist = (word.li.text)\n",
    "            with open('Movies_List.txt','+a',encoding='utf-8') as file:\n",
    "                file.write(movieslist)\n",
    "                file.write(\"\\n\")\n",
    "    next_pg_link = (soup.find('a',class_='page-link',rel= 'next').get('href'))\n",
    "    print(next_pg_link)\n",
    "    return (next_pg_link)\n",
    "\n",
    "c = fun(a)\n",
    "for i in range(5):\n",
    "    c = fun(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22c7ba",
   "metadata": {},
   "source": [
    "### Learning how to add multiple pages with alternate method\n",
    "You can also create a shortcut by manupilating the link https://subslikescript.com/movies?page=2 just change the number here with for loop and i\n",
    "\n",
    "# Here we extracting next pages by manipulating the url by breaking in first, middle, parts and extract only chaning parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "837365d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.in/s?k=samsung&page=2&crid=35TX31OAMPQY1&sprefix=samsung%2Caps%2C282&ref=nb_sb_noss_1\n",
      "https://www.amazon.in/s?k=samsung&page=3&crid=35TX31OAMPQY1&sprefix=samsung%2Caps%2C282&ref=nb_sb_noss_1\n",
      "https://www.amazon.in/s?k=samsung&page=4&crid=35TX31OAMPQY1&sprefix=samsung%2Caps%2C282&ref=nb_sb_noss_1\n",
      "https://www.amazon.in/s?k=samsung&page=5&crid=35TX31OAMPQY1&sprefix=samsung%2Caps%2C282&ref=nb_sb_noss_1\n",
      "https://www.amazon.in/s?k=samsung&page=6&crid=35TX31OAMPQY1&sprefix=samsung%2Caps%2C282&ref=nb_sb_noss_1\n",
      "https://www.amazon.in/s?k=samsung&page=7&crid=35TX31OAMPQY1&sprefix=samsung%2Caps%2C282&ref=nb_sb_noss_1\n",
      "https://www.amazon.in/s?k=samsung&page=8&crid=35TX31OAMPQY1&sprefix=samsung%2Caps%2C282&ref=nb_sb_noss_1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     soup \u001b[38;5;241m=\u001b[39m getdata(url)\n\u001b[1;32m---> 22\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[43mnext_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [23], line 16\u001b[0m, in \u001b[0;36mnext_page\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext_page\u001b[39m(soup):\n\u001b[0;32m     15\u001b[0m     afd \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m,class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms-pagination-strip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     middle \u001b[38;5;241m=\u001b[39m \u001b[43mafd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m,class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms-pagination-item s-pagination-next s-pagination-button s-pagination-separator\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m     nexturl \u001b[38;5;241m=\u001b[39m (first\u001b[38;5;241m+\u001b[39mmiddle\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&crid=35TX31OAMPQY1&sprefix=samsung\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2Caps\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m2C282&ref=nb_sb_noss_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nexturl\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url = 'https://www.amazon.in/s?k=samsung&crid=35TX31OAMPQY1&sprefix=samsung%2Caps%2C282&ref=nb_sb_noss_1'\n",
    "#Extract data containig all amazon product name with samsung with all pages\n",
    "\n",
    "def getdata(url):\n",
    "    content = requests.get(url).text\n",
    "    soup = BeautifulSoup(content,'lxml')\n",
    "    return soup\n",
    "\n",
    "first = 'https://www.amazon.in'\n",
    "def next_page(soup):\n",
    "    afd = soup.find('span',class_='s-pagination-strip')\n",
    "    middle = afd.find('a',class_='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator')['href']\n",
    "    nexturl = (first+middle+\"&crid=35TX31OAMPQY1&sprefix=samsung%2Caps%2C282&ref=nb_sb_noss_1\")\n",
    "    return nexturl\n",
    "    \n",
    "while True:\n",
    "    soup = getdata(url)\n",
    "    url = next_page(soup)\n",
    "    if not url:\n",
    "        break\n",
    "    print(url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d3283b7ede15a3ba02d29b57611833951a6474bfc2330b92af0513ee46fd488"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
