{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a0c698",
   "metadata": {},
   "source": [
    "## Section 1: Scraping a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af533970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic (1997) - full transcript\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Collecting data from html is done by request\n",
    "#############################################\n",
    "\n",
    "\"\"\" \n",
    "Requesting the content from website using Requests. \n",
    "requests.get is for sending Response code\n",
    "We are not required the status code but we are requiring the content i.e. the text inside it so use .text\n",
    "\"\"\"\n",
    "\n",
    "website = 'https://subslikescript.com/movie/Titanic-120338'\n",
    "result = requests.get(website)\n",
    "content = result.text\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# BeautifulSoup is for to process pull data from html \n",
    "#####################################################\n",
    "\n",
    "\"\"\"\n",
    "Beautiful Soup is like a big bowl which organize content for fast extraction making scrapping task easy\n",
    "So our soup is main object where all operation has to perform\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(content,'lxml')\n",
    "# print(soup.prettify())\n",
    "\n",
    "\n",
    "title = soup.find('article',class_=\"main-article\")\n",
    "\n",
    "\n",
    "# print(title.h1.text.split()[:-3])\n",
    "name = title.h1.text.split()[:-3]\n",
    "\n",
    "\n",
    "movie_name = ' '.join(name)\n",
    "print(movie_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "#  Exporting data in a text file\n",
    "################################\n",
    "\n",
    "with open(f'{movie_name}.txt','w') as file:\n",
    "          file.write(movie_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b58f46",
   "metadata": {},
   "source": [
    "## Section 2: Scraping Multiple Transcripts and Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0447563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://subslikescript.com/movies?page=2\n",
      "https://subslikescript.com/movies?page=3\n",
      "https://subslikescript.com/movies?page=4\n",
      "https://subslikescript.com/movies?page=5\n",
      "https://subslikescript.com/movies?page=6\n",
      "https://subslikescript.com/movies?page=7\n",
      "https://subslikescript.com/movies?page=8\n",
      "https://subslikescript.com/movies?page=9\n",
      "https://subslikescript.com/movies?page=10\n",
      "https://subslikescript.com/movies?page=11\n",
      "https://subslikescript.com/movies?page=12\n",
      "https://subslikescript.com/movies?page=13\n",
      "https://subslikescript.com/movies?page=14\n"
     ]
    }
   ],
   "source": [
    "Movies_list = requests.get('https://subslikescript.com/movies').text\n",
    "soup1 = BeautifulSoup(Movies_list,'lxml')\n",
    "a = (soup1.find('a',class_='page-link').get('href'))\n",
    "print(a)\n",
    "\n",
    "\n",
    "def fun(link):\n",
    "    m = requests.get(link).text\n",
    "    soup = BeautifulSoup(m,'lxml')\n",
    "    next_pg_link = (soup.find('a',class_='page-link',rel= 'next').get('href'))\n",
    "    print(next_pg_link)\n",
    "    return (next_pg_link)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c = fun(a)\n",
    "for i in range(11):\n",
    "    c = fun(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afab918",
   "metadata": {},
   "source": [
    "### Now collecting movies name on single page \n",
    "Learn why we use find_all over find if we wanted to iterate over multiple similar type tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e125b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flaxy Martin (1949)\n",
      "Smart Girls Don't Talk (1948)\n",
      "Kieu (2021)\n",
      "Call Me Victor (1993)\n",
      "Eva & Adam (2021)\n",
      "The Loser Hero (2018)\n",
      "Andrew Santino: Cheeseburger (2023)\n",
      "My Sister's Serial Killer Boyfriend (2023)\n",
      "Yoba (1976)\n",
      "Trail to Terror (2016)\n",
      "Beautiful Minds (2021)\n",
      "Utama (2022)\n",
      "Savage Messiah (2002)\n",
      "Eddie Murphy: Delirious (1983)\n",
      "Gila! (2012)\n",
      "Bed Rest (2022)\n",
      "Ju-on: White Ghost (2009)\n",
      "D Block (2022)\n",
      "XxxHolic (2022)\n",
      "#DUPE# (2007)\n",
      "The Road to Denver (1955)\n",
      "Trail Street (1947)\n",
      "Bhaskar the Rascal (2015)\n",
      "The Hero (2017)\n",
      "The Meteor Man (1993)\n",
      "Minazuki (1999)\n",
      "Heartland of Darkness (1992)\n",
      "The Tin Soldier (1995)\n",
      "Joyland (2022)\n",
      "Reba McEntire's the Hammer (2023)\n"
     ]
    }
   ],
   "source": [
    "# Function to collect movies list of single page\n",
    "\n",
    "\n",
    "######################\n",
    "## Collecting movies name using find\n",
    "######################\n",
    "# u = soup1.find('ul',class_='scripts-list').a.li\n",
    "# print(u)\n",
    "\n",
    "\"\"\"We saw using find we are able to extract only single name of movie over multiple elements \n",
    "so we are using find_all bcoz it's not possible to use for loop for u here and extract movies name using a.li.text \n",
    "for i in u:\n",
    "    print(i.a.li) #this is not correct method bcoz i is iterating over each item in u\n",
    "\"\"\"   \n",
    "\n",
    "def finding_list():\n",
    "    for word in soup1.find_all('a'):\n",
    "        if word.li != None:\n",
    "            print(word.li.text)\n",
    "finding_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2d5f7",
   "metadata": {},
   "source": [
    "### Adding all above code in one block and creating a txt file with all movies name with link at top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda85f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://subslikescript.com/movies?page=2\n",
      "https://subslikescript.com/movies?page=3\n",
      "https://subslikescript.com/movies?page=4\n",
      "https://subslikescript.com/movies?page=5\n",
      "https://subslikescript.com/movies?page=6\n",
      "https://subslikescript.com/movies?page=7\n",
      "https://subslikescript.com/movies?page=8\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "\n",
    "Movies_list = requests.get('https://subslikescript.com/movies').text\n",
    "soup1 = BeautifulSoup(Movies_list,'lxml')\n",
    "a = (soup1.find('a',class_='page-link').get('href'))\n",
    "print(a)\n",
    "\n",
    "\n",
    "def fun(link):\n",
    "    m = requests.get(link).text\n",
    "    soup = BeautifulSoup(m,'lxml')\n",
    "    for word in soup.find_all('a'):\n",
    "        if word.li != None:\n",
    "            movieslist = (word.li.text)\n",
    "            with open('Movies_List.txt','+a',encoding='utf-8') as file:\n",
    "                file.write(movieslist)\n",
    "                file.write(\"\\n\")\n",
    "    next_pg_link = (soup.find('a',class_='page-link',rel= 'next').get('href'))\n",
    "    print(next_pg_link)\n",
    "    return (next_pg_link)\n",
    "\n",
    "c = fun(a)\n",
    "for i in range(5):\n",
    "    c = fun(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22c7ba",
   "metadata": {},
   "source": [
    "### Learning how to add multiple pages with alternate method\n",
    "You can also create a shortcut by manupilating the link https://subslikescript.com/movies?page=2 just change the number here with for loop and i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837365d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"s-pagination-strip\"><span aria-disabled=\"true\" class=\"s-pagination-item s-pagination-previous s-pagination-disabled\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"12\" viewbox=\"0 0 8 12\" width=\"8\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M5.874.35a1.28 1.28 0 011.761 0 1.165 1.165 0 010 1.695L3.522 6l4.113 3.955a1.165 1.165 0 010 1.694 1.28 1.28 0 01-1.76 0L0 6 5.874.35z\"></path></svg>Previous</span><script>P.declare('s\\-optimized\\-pagination\\-config', {\"prefetchTargetIndex\":0,\"enabled\":false});</script><span aria-label=\"Current page, page 1\" class=\"s-pagination-item s-pagination-selected\">1</span><a aria-label=\"Go to page 2\" class=\"s-pagination-item s-pagination-button\" href=\"/s?k=samsung&amp;page=2\">2</a><a aria-label=\"Go to page 3\" class=\"s-pagination-item s-pagination-button\" href=\"/s?k=samsung&amp;page=3\">3</a><span aria-hidden=\"true\" class=\"s-pagination-item s-pagination-ellipsis\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"2\" viewbox=\"0 0 10 2\" width=\"10\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M9 2c-.608 0-1-.425-1-1s.392-1 1-1 1 .448 1 1c0 .575-.392 1-1 1zM5 2c-.608 0-1-.425-1-1s.392-1 1-1 1 .448 1 1c0 .575-.392 1-1 1zM1 2c-.608 0-1-.425-1-1s.392-1 1-1 1 .448 1 1c0 .575-.392 1-1 1z\"></path>...</svg></span><span aria-disabled=\"true\" class=\"s-pagination-item s-pagination-disabled\">20</span><a aria-label=\"Go to next page, page 2\" class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\" href=\"/s?k=samsung&amp;page=2\">Next<svg aria-hidden=\"true\" focusable=\"false\" height=\"12\" viewbox=\"0 0 8 12\" width=\"8\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M2.126.35a1.28 1.28 0 00-1.761 0 1.165 1.165 0 000 1.695L4.478 6 .365 9.955a1.165 1.165 0 000 1.694 1.28 1.28 0 001.76 0L8 6 2.126.35z\"></path></svg></a></span>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "content = requests.get('https://www.amazon.in/s?k=samsung&crid=35TX31OAMPQY1&sprefix=samsung%2Caps%2C282&ref=nb_sb_noss_1').text\n",
    "soup = BeautifulSoup(content,'lxml')\n",
    "\n",
    "afd = soup.find('span',class_='s-pagination-strip')\n",
    "print(afd.find('sp'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d3283b7ede15a3ba02d29b57611833951a6474bfc2330b92af0513ee46fd488"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
