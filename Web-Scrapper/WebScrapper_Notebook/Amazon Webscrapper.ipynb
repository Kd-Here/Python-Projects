{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e02ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all files\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2f84db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redmi 10A (Slate Grey, 4GB RAM, 64GB Storage) | 2 Ghz Octa Core Helio G25 | 5000 mAh Battery | Finger Print Sensor | Upto 5GB RAM with RAM Booster\n",
      "â‚¹8,299.00\n"
     ]
    }
   ],
   "source": [
    "#Connecting with website\n",
    "\n",
    "\n",
    "url = 'https://www.amazon.in/Redmi-Storage-Battery-Finger-Booster/dp/B09XB7SRQ5/ref=pd_rhf_d_dp_s_pd_crcbs_sccl_2_3/261-1490718-8429552?pd_rd_w=6cMW1&content-id=amzn1.sym.5d8cdd8d-be53-4391-b82d-376d461d85f0&pf_rd_p=5d8cdd8d-be53-4391-b82d-376d461d85f0&pf_rd_r=TNRCS3307TAK5YC28S27&pd_rd_wg=WRSqG&pd_rd_r=152c2c91-83d9-4cf8-b177-ebd4666cf85c&pd_rd_i=B09XB7SRQ5&psc=1'\n",
    "# https://httpbin.org/get \n",
    "# From this link you get your User-Agent info\n",
    "private_info = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
    "\n",
    "\n",
    "page = requests.get(url,headers = private_info)\n",
    "\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "# soup2 = soup.prettify()\n",
    "# print(soup2)\n",
    "\n",
    "\n",
    "product_title = soup.find(id='productTitle').text.strip()\n",
    "print(product_title.strip())\n",
    "price = soup.find(class_=\"a-offscreen\").text.strip()\n",
    "print(price.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d27ffefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Product','Price']\n",
    "data = [product_title,price]\n",
    "\n",
    "# with open('products.csv', 'w', newline='',encoding=\"utf-8\") as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(header)\n",
    "#     writer.writerow(data)\n",
    "# # w is representing write function but if you wanted to store the data in periodic format use append\n",
    "\n",
    "with open('products.csv', 'a+', newline='',encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0c698",
   "metadata": {},
   "source": [
    "## Section 1: Scraping a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af533970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic (1997)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Collecting data from html is done by request\n",
    "#############################################\n",
    "\n",
    "\"\"\" \n",
    "Requesting the content from website using Requests. \n",
    "requests.get is for sending Response code\n",
    "We are not required the status code but we are requiring the content i.e. the text inside it so use .text\n",
    "\"\"\"\n",
    "\n",
    "website = 'https://subslikescript.com/movie/Titanic-120338'\n",
    "result = requests.get(website)\n",
    "content = result.text\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# BeautifulSoup is for to process pull data from html \n",
    "#####################################################\n",
    "\n",
    "\"\"\"\n",
    "Beautiful Soup is like a big bowl which organize content for fast extraction making scrapping task easy\n",
    "So our soup is main object where all operation has to perform\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(content,'lxml')\n",
    "# print(soup.prettify())\n",
    "\n",
    "\n",
    "title = soup.find('article',class_=\"main-article\")\n",
    "\n",
    "\n",
    "# print(title.h1.text.split()[:-3])\n",
    "name = title.h1.text.split()[:-3]\n",
    "\n",
    "\n",
    "movie_name = ' '.join(name)\n",
    "print(movie_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "#  Exporting data in a text file\n",
    "################################\n",
    "\n",
    "with open(f'{movie_name}.txt','w') as file:\n",
    "          file.write(movie_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b58f46",
   "metadata": {},
   "source": [
    "## Section 2: Scraping Multiple Transcripts and Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d0447563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://subslikescript.com/movies?page=2\n",
      "https://subslikescript.com/movies?page=3\n",
      "https://subslikescript.com/movies?page=4\n",
      "https://subslikescript.com/movies?page=5\n",
      "https://subslikescript.com/movies?page=6\n",
      "https://subslikescript.com/movies?page=7\n",
      "https://subslikescript.com/movies?page=8\n",
      "https://subslikescript.com/movies?page=9\n",
      "https://subslikescript.com/movies?page=10\n",
      "https://subslikescript.com/movies?page=11\n",
      "https://subslikescript.com/movies?page=12\n",
      "https://subslikescript.com/movies?page=13\n",
      "https://subslikescript.com/movies?page=14\n"
     ]
    }
   ],
   "source": [
    "Movies_list = requests.get('https://subslikescript.com/movies').text\n",
    "soup1 = BeautifulSoup(Movies_list,'lxml')\n",
    "a = (soup1.find('a',class_='page-link').get('href'))\n",
    "print(a)\n",
    "c = fun(a)\n",
    "\n",
    "\n",
    "def fun(link):\n",
    "    m = requests.get(link).text\n",
    "    soup = BeautifulSoup(m,'lxml')\n",
    "    next_pg_link = (soup.find('a',class_='page-link',rel= 'next').get('href'))\n",
    "    print(next_pg_link)\n",
    "    return (next_pg_link)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(11):\n",
    "    c = fun(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afab918",
   "metadata": {},
   "source": [
    "### Now collecting movies name on single page \n",
    "Learn why we use find_all over find if we wanted to iterate over multiple similar type tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d9e125b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flaxy Martin (1949)\n",
      "Smart Girls Don't Talk (1948)\n",
      "Kieu (2021)\n",
      "Call Me Victor (1993)\n",
      "Eva & Adam (2021)\n",
      "The Loser Hero (2018)\n",
      "Andrew Santino: Cheeseburger (2023)\n",
      "My Sister's Serial Killer Boyfriend (2023)\n",
      "Yoba (1976)\n",
      "Trail to Terror (2016)\n",
      "Beautiful Minds (2021)\n",
      "Utama (2022)\n",
      "Savage Messiah (2002)\n",
      "Eddie Murphy: Delirious (1983)\n",
      "Gila! (2012)\n",
      "Bed Rest (2022)\n",
      "Ju-on: White Ghost (2009)\n",
      "D Block (2022)\n",
      "XxxHolic (2022)\n",
      "#DUPE# (2007)\n",
      "The Road to Denver (1955)\n",
      "Trail Street (1947)\n",
      "Bhaskar the Rascal (2015)\n",
      "The Hero (2017)\n",
      "The Meteor Man (1993)\n",
      "Minazuki (1999)\n",
      "Heartland of Darkness (1992)\n",
      "The Tin Soldier (1995)\n",
      "Joyland (2022)\n",
      "Reba McEntire's the Hammer (2023)\n"
     ]
    }
   ],
   "source": [
    "# Function to collect movies list of single page\n",
    "\n",
    "\n",
    "######################\n",
    "## Collecting movies name using find\n",
    "######################\n",
    "# u = soup1.find('ul',class_='scripts-list').a.li\n",
    "# print(u)\n",
    "\n",
    "\"\"\"We saw using find we are able to extract only single name of movie over multiple elements \n",
    "so we are using find_all bcoz it's not possible to use for loop for u here and extract movies name using a.li.text \n",
    "for i in u:\n",
    "    print(i.a.li) #this is not correct method bcoz i is iterating over each item in u\n",
    "\"\"\"   \n",
    "\n",
    "def finding_list():\n",
    "    for word in soup1.find_all('a'):\n",
    "        if word.li != None:\n",
    "            print(word.li.text)\n",
    "finding_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2d5f7",
   "metadata": {},
   "source": [
    "### Adding all above code in one block and creating a txt file with all movies name with link at top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dda85f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://subslikescript.com/movies?page=2\n",
      "EO (2022)\n",
      "Please Believe Me (1950)\n",
      "Devil's Canyon (1953)\n",
      "Those Who Call (2021)\n",
      "Scream of the Wolf (2022)\n",
      "Alai Payuthey (2000)\n",
      "A Force More Powerful (1999)\n",
      "Bringing Down a Dictator (2002)\n",
      "As You Were (2014)\n",
      "The Cordillera of Dreams (2019)\n",
      "Drunken Master III (1994)\n",
      "The Madness (2020)\n",
      "Freeway Killer (2010)\n",
      "The Christmas Card (2006)\n",
      "Blood and Diamonds (1977)\n",
      "The Santa Trap (2002)\n",
      "Just the Ticket (1998)\n",
      "Amityville Death House (2015)\n",
      "Amityville Scarecrow 2 (2022)\n",
      "Nabbie no koi (1999)\n",
      "Camps of the Dead (1947)\n",
      "The Walking Hills (1949)\n",
      "Amityville in Space (2022)\n",
      "A Day for Thanks on Walton's Mountain (1982)\n",
      "The Living and the Dead (2006)\n",
      "The Woman on the Beach (1947)\n",
      "The Habit of Beauty (2016)\n",
      "The Adventures of Baron Munchausen (1988)\n",
      "Talbis Iblis (2022)\n",
      "Babe Watch: Forbidden Parody (1996)\n",
      "https://subslikescript.com/movies?page=3\n",
      "Izakaya ChÃ´ji (1983)\n",
      "Le coeur noir des forÃªts (2021)\n",
      "The Long Way Home (1998)\n",
      "Madam Satan (1930)\n",
      "Sex with Sue (2022)\n",
      "13:14. The Challenge of Helping (2022)\n",
      "Yugi (2022)\n",
      "Silent Screamplay II (2006)\n",
      "Jingle Hell (2000)\n",
      "Skylark (1941)\n",
      "Flux Gourmet (2022)\n",
      "Pevnost (1994)\n",
      "Secrets in the Building (2022)\n",
      "Come Find Me (2021)\n",
      "The Mummy: Resurrection (2022)\n",
      "Happy F'K'IN Sunshine (2020)\n",
      "Eine Stunde GlÃ¼ck (1931)\n",
      "A Broken Drum (1949)\n",
      "Hanzo the Razor: The Snare (1973)\n",
      "Hana chirinu (1938)\n",
      "Wakai hito (1937)\n",
      "Enoken no chakkiri Kinta 'Zen' - Mamayo sandogasa - Ikiwa yoiyoi (1937)\n",
      "GlÃ¼ckskinder (1936)\n",
      "Les amours de minuit (1931)\n",
      "Peter the First (1937)\n",
      "OjÃ´san kanpai (1949)\n",
      "Saving Grace (1986)\n",
      "How to Start a Revolution (2011)\n",
      "Mother Lode (1982)\n",
      "Return to the 36th Chamber (1980)\n",
      "https://subslikescript.com/movies?page=4\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "\n",
    "Movies_list = requests.get('https://subslikescript.com/movies').text\n",
    "soup1 = BeautifulSoup(Movies_list,'lxml')\n",
    "a = (soup1.find('a',class_='page-link').get('href'))\n",
    "print(a)\n",
    "\n",
    "def finding_list(link):\n",
    "    for word in link.find_all('a'):\n",
    "        if word.li != None:\n",
    "            print(word.li.text)\n",
    "\n",
    "\n",
    "def fun(link):\n",
    "    m = requests.get(link).text\n",
    "    soup = BeautifulSoup(m,'lxml')\n",
    "    finding_list(soup)\n",
    "    next_pg_link = (soup.find('a',class_='page-link',rel= 'next').get('href'))\n",
    "    with open('Movies_List.txt','w') as file:\n",
    "        file.write\n",
    "    print(next_pg_link)\n",
    "    return (next_pg_link)\n",
    "    \n",
    "\n",
    "\n",
    "c = fun(a)\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    c = fun(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
