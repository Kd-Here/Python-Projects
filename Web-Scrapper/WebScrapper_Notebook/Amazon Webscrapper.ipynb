{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e02ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all files\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "import smtplib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2f84db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redmi 10A (Slate Grey, 4GB RAM, 64GB Storage) | 2 Ghz Octa Core Helio G25 | 5000 mAh Battery | Finger Print Sensor | Upto 5GB RAM with RAM Booster\n",
      "â‚¹8,299.00\n"
     ]
    }
   ],
   "source": [
    "#Connecting with website\n",
    "\n",
    "\n",
    "url = 'https://www.amazon.in/Redmi-Storage-Battery-Finger-Booster/dp/B09XB7SRQ5/ref=pd_rhf_d_dp_s_pd_crcbs_sccl_2_3/261-1490718-8429552?pd_rd_w=6cMW1&content-id=amzn1.sym.5d8cdd8d-be53-4391-b82d-376d461d85f0&pf_rd_p=5d8cdd8d-be53-4391-b82d-376d461d85f0&pf_rd_r=TNRCS3307TAK5YC28S27&pd_rd_wg=WRSqG&pd_rd_r=152c2c91-83d9-4cf8-b177-ebd4666cf85c&pd_rd_i=B09XB7SRQ5&psc=1'\n",
    "# https://httpbin.org/get \n",
    "# From this link you get your User-Agent info\n",
    "private_info = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
    "\n",
    "\n",
    "page = requests.get(url,headers = private_info)\n",
    "\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "# soup2 = soup.prettify()\n",
    "# print(soup2)\n",
    "\n",
    "\n",
    "product_title = soup.find(id='productTitle').text.strip()\n",
    "print(product_title.strip())\n",
    "price = soup.find(class_=\"a-offscreen\").text.strip()\n",
    "print(price.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d27ffefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Product','Price']\n",
    "data = [product_title,price]\n",
    "\n",
    "# with open('products.csv', 'w', newline='',encoding=\"utf-8\") as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(header)\n",
    "#     writer.writerow(data)\n",
    "# # w is representing write function but if you wanted to store the data in periodic format use append\n",
    "\n",
    "with open('products.csv', 'a+', newline='',encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0c698",
   "metadata": {},
   "source": [
    "## Section 1: Scraping a single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af533970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic (1997)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Collecting data from html is done by request\n",
    "#############################################\n",
    "\n",
    "\"\"\" \n",
    "Requesting the content from website using Requests. \n",
    "requests.get is for sending Response code\n",
    "We are not required the status code but we are requiring the content i.e. the text inside it so use .text\n",
    "\"\"\"\n",
    "\n",
    "website = 'https://subslikescript.com/movie/Titanic-120338'\n",
    "result = requests.get(website)\n",
    "content = result.text\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# BeautifulSoup is for to process pull data from html \n",
    "#####################################################\n",
    "\n",
    "\"\"\"\n",
    "Beautiful Soup is like a big bowl which organize content for fast extraction making scrapping task easy\n",
    "So our soup is main object where all operation has to perform\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(content,'lxml')\n",
    "# print(soup.prettify())\n",
    "\n",
    "\n",
    "title = soup.find('article',class_=\"main-article\")\n",
    "\n",
    "\n",
    "# print(title.h1.text.split()[:-3])\n",
    "name = title.h1.text.split()[:-3]\n",
    "\n",
    "\n",
    "movie_name = ' '.join(name)\n",
    "print(movie_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "#  Exporting data in a text file\n",
    "################################\n",
    "\n",
    "with open(f'{movie_name}.txt','w') as file:\n",
    "          file.write(movie_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b58f46",
   "metadata": {},
   "source": [
    "## Section 2: Scraping Multiple Transcripts and Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d0447563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://subslikescript.com/movies?page=2\n",
      "https://subslikescript.com/movies?page=3\n",
      "https://subslikescript.com/movies?page=4\n",
      "https://subslikescript.com/movies?page=5\n"
     ]
    }
   ],
   "source": [
    "Movies_list = requests.get('https://subslikescript.com/movies').text\n",
    "soup1 = BeautifulSoup(Movies_list,'lxml')\n",
    "a = (soup1.find('a',class_='page-link',rel= 'next').get('href'))\n",
    "print(a)\n",
    "c = fun(a)\n",
    "\n",
    "\n",
    "def fun(link):\n",
    "    m = requests.get(link).text\n",
    "    soup = BeautifulSoup(m,'lxml')\n",
    "    nee = (soup.find('a',class_='page-link',rel= 'next').get('href'))\n",
    "    print(nee)\n",
    "    return (nee)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    c = fun(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4937bf",
   "metadata": {},
   "source": [
    "### Now collecting movies name on single page \n",
    "Learn why we use find_all over find if we wanted to iterate over multiple similar type tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d9e125b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li>Flaxy Martin (1949)</li>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"We saw using find we are able to extract only single name of movie over multiple elements so we are using find_all bcoz it's not possible to use for loop for u here and extract movies name using a.li.text\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to collect movies list of single page\n",
    "\n",
    "\n",
    "# #####################\n",
    "## Collecting movies name using find\n",
    "# #####################\n",
    "\n",
    "u = soup1.find('ul',class_='scripts-list').a.li\n",
    "print(u)\n",
    "\n",
    "\"\"\"We saw using find we are able to extract only single name of movie over multiple elements \n",
    "so we are using find_all bcoz it's not possible to use for loop for u here and extract movies name using a.li.text \n",
    "for i in u:\n",
    "    \n",
    "\"\"\"   \n",
    "\n",
    "# def finding_list():\n",
    "#     for word in soup1.find_all('a'):\n",
    "#         if word.li != None:\n",
    "#             print(word.li.text)\n",
    "# finding_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda85f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
